# -*- coding: utf-8 -*-
"""TUBES DATA MINING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ay0q9i4qIag4_HGnGwjm2_jBTKCywVRz

# **TUGAS BESAR MK DATA MINING**
**KELOMPOK I**

*   Indah Rizki Rahmawati (F55121004)
*   Sahron Angelina Ihalauw (F55121030)

Tugas ini merupakan tugas besar untuk matakuliah Data Mining.:

Semester 6

Prodi Teknik Informatika

Universitas Tadulako
"""

"""# **A. Regression & Clustering**

**1. Crawling/Sracping Data dari sumber data berikut [Data Perkiraan Cuaca Terbuka BMKG](https://data.bmkg.go.id/prakiraan-cuaca/)**

Memilih cuaca terekstream
"""

#core Pkg
import streamlit as st
import streamlit.components.v1 as stc
from PIL import image

# Unggah file XML
from google.colab import files
uploaded = files.upload()

import xml.etree.ElementTree as ET

# Memuat file XML
tree = ET.parse('DigitalForecast-KalimantanTengah.xml')
root = tree.getroot()

# Debugging: Cetak struktur XML
def print_element(element, level=0):
    indent = "  " * level
    print(f"{indent}{element.tag} {element.attrib}")
    for subelement in element:
        print_element(subelement, level + 1)

print_element(root)

import pandas as pd

# Inisialisasi list kosong untuk menyimpan data
data = []

# Mengiterasi elemen 'area' dan menyimpan data ke dalam list
for area in root.findall('.//area'):
    area_id = area.get('id')
    area_latitude = area.get('latitude')
    area_longitude = area.get('longitude')
    area_description = area.get('description')
    area_domain = area.get('domain')

    for parameter in area.findall('parameter'):
        parameter_id = parameter.get('id')
        parameter_description = parameter.get('description')
        parameter_type = parameter.get('type')

        for timerange in parameter.findall('timerange'):
            timerange_type = timerange.get('type')
            timerange_h = timerange.get('h')
            timerange_day = timerange.get('day')
            timerange_datetime = timerange.get('datetime')

            for value in timerange.findall('value'):
                value_unit = value.get('unit')
                value_text = value.text

                data.append({
                    'area_id': area_id,
                    'area_latitude': area_latitude,
                    'area_longitude': area_longitude,
                    'area_description': area_description,
                    'area_domain': area_domain,
                    'parameter_id': parameter_id,
                    'parameter_description': parameter_description,
                    'parameter_type': parameter_type,
                    'timerange_type': timerange_type,
                    'timerange_h': timerange_h,
                    'timerange_day': timerange_day,
                    'timerange_datetime': timerange_datetime,
                    'value_unit': value_unit,
                    'value_text': value_text
                })

# Membuat DataFrame dari data
df = pd.DataFrame(data)

# Tampilkan DataFrame
df.head()

# Inisialisasi dictionary untuk menyimpan DataFrame terpisah berdasarkan parameter
parameter_dfs = {}

# Mengiterasi setiap parameter unik dan memisahkan data
for parameter_id in df['parameter_id'].unique():
    parameter_df = df[df['parameter_id'] == parameter_id].copy()
    parameter_df = parameter_df[['area_id', 'area_description', 'timerange_datetime', 'value_text']].pivot_table(
        index=['area_id', 'area_description', 'timerange_datetime'],
        values='value_text',
        aggfunc='first'
    ).reset_index()
    parameter_dfs[parameter_id] = parameter_df

# Tampilkan DataFrame terpisah untuk parameter humidity sebagai contoh
humidity_df = parameter_dfs['hu']
humidity_df.head()

# Mulai dengan DataFrame untuk parameter pertama
combined_df = parameter_dfs[list(parameter_dfs.keys())[0]]

# Gabungkan dengan DataFrame parameter lainnya
for parameter_id in list(parameter_dfs.keys())[1:]:
    combined_df = pd.merge(combined_df, parameter_dfs[parameter_id],
                           on=['area_id', 'area_description', 'timerange_datetime'],
                           how='outer', suffixes=('', f'_{parameter_id}'))

# Tampilkan DataFrame yang telah digabungkan
combined_df.head()

combined_df.to_csv('data_scraped.csv', index=False)

from google.colab import files

# Mengunduh file CSV
files.download('data_scraped.csv')

"""**2. Proses EDA**"""

df.head()

#Menampilkan ringkasan informasi tentang DataFrame
df.info()

# Tampilkan statistik deskriptif
df.describe()

#menghitung jumlah baris yang duplikat dalam DataFrame
df.duplicated().sum()

# Mengecek missing values
df.isnull().sum()

#menghitung frekuensi masing-masing nilai unik dalam kolom value_text
df['value_text'].value_counts()

# Line plot untuk melihat perubahan nilai 'value_text' terhadap 'timerange_datetime'
import matplotlib.pyplot as plt
import seaborn as sns


plt.figure(figsize=(12, 6))
sns.lineplot(x='timerange_datetime', y='value_text', hue='parameter_type', data=df)
plt.title('Perubahan Nilai Parameter terhadap Waktu')
plt.xlabel('Waktu')
plt.ylabel('Nilai Parameter')
plt.xticks(rotation=45)
plt.legend(loc='upper right')
plt.tight_layout()

"""**3. Insight**

**Ringkasan Informasi DataFrame:**
*   DataFrame memiliki 2100 baris dan 14 kolom,
semua kolom memiliki tipe data object.
*   Terdapat kolom timerange_h yang memiliki 252 nilai null, dan kolom timerange_day yang memiliki 1848 nilai null.

**Statistik Deskriptif:**
Karena semua kolom memiliki tipe data object, perhitungan statistik deskriptif seperti mean, std, min, dan max tidak relevan dalam konteks ini. Ini menunjukkan perlu untuk mengonversi beberapa kolom menjadi tipe data numerik jika diperlukan untuk analisis lebih lanjut.

**Duplikasi Baris**
Tidak ada baris yang duplikat dalam DataFrame ini (df.duplicated().sum() = 0), yang menunjukkan data murni tanpa duplikasi.

**Missing Values:**
Kolom timerange_h memiliki 252 nilai null, sedangkan kolom timerange_day memiliki 1848 nilai null. Ini menunjukkan bahwa kolom timerange_day memiliki sebagian besar nilai kosong dan perlu dipertimbangkan bagaimana menangani nilai yang hilang ini.

**Frekuensi Nilai Unik dalam Kolom 'value_text':
Kolom value_text memiliki 74 nilai unik dengan frekuensi masing-masing nilai yang bervariasi dari 1 hingga 162. Nilai 0 adalah nilai yang paling sering muncul dengan frekuensi 162.

**Insight Yang Dapat Diambil:**
*   Data Quality: Perlu perhatian khusus terhadap kolom timerange_day yang memiliki banyak nilai null. Mungkin perlu dilakukan pengecekan lebih lanjut apakah kolom ini memang harus diisi atau ada strategi penggantian yang lebih baik.
*   Nilai Dominan: Nilai 0 dalam kolom value_text mendominasi, dengan kemungkinan besar mempengaruhi analisis statistik atau model yang akan dibangun nantinya.
*   Pemrosesan Lanjutan: Kemungkinan perlu untuk mengonversi kolom-kolom yang relevan menjadi tipe data numerik untuk memungkinkan analisis statistik lebih lanjut.
*   Kesimpulan: EDA awal menunjukkan bahwa data memiliki beberapa aspek yang perlu diperbaiki sebelum analisis lebih lanjut. Hal ini termasuk menangani nilai null, memastikan representasi yang tepat dari data (misalnya, mengubah tipe data jika diperlukan), dan memahami distribusi nilai dalam setiap kolom untuk mempersiapkan data untuk tahap analisis berikutnya.

**4. Prediksi temperatur/suhu rata-rata 30 hari setelahnya dari tanggal terakhir**
"""

import pandas as pd
from datetime import datetime, timedelta

# Contoh data (gantilah dengan data yang sesuai)
data = {
    'timerange_datetime': pd.date_range('2024-05-24', periods=100),
    'temperature': [25.0 + i * 0.5 for i in range(100)]  # Data suhu contoh
}

combined_df = pd.DataFrame(data)

# Pastikan kolom 'timerange_datetime' sudah dalam format datetime
combined_df['timerange_datetime'] = pd.to_datetime(combined_df['timerange_datetime'])

# Tentukan tanggal mulai prediksi
start_date = datetime(2024, 6, 23)

# Hitung tanggal 30 hari setelah start_date
end_date_prediction = start_date + timedelta(days=30)

# Filter data untuk 30 hari sebelum start_date
data_for_last_30_days = combined_df[(combined_df['timerange_datetime'] > start_date - timedelta(days=30)) &
                                    (combined_df['timerange_datetime'] <= start_date)]

# Prediksi suhu rata-rata berdasarkan data 30 hari terakhir
mean_temperature_last_30_days = data_for_last_30_days['temperature'].mean()

print(f"Prediksi suhu rata-rata untuk 30 hari ke depan dari {start_date.date()} adalah {mean_temperature_last_30_days:.2f} derajat.")

"""Untuk melakukan prediksi suhu rata-rata 30 hari ke depan, langkah-langkah berikut dilakukan menggunakan Python dan Pandas. Pertama, data historis suhu dimuat ke dalam DataFrame dengan kolom 'timerange_datetime' untuk tanggal dan 'temperature' untuk nilai suhu. Kemudian, tanggal awal prediksi ditentukan dan dihitung tanggal akhir prediksi dengan menambahkan 30 hari ke tanggal awal. Data untuk 30 hari terakhir sebelum tanggal awal prediksi difilter dari dataset untuk memastikan prediksi berdasarkan data yang relevan. Selanjutnya, suhu rata-rata dari data yang difilter dihitung dan diumumkan sebagai prediksi suhu rata-rata untuk 30 hari mendatang dari tanggal yang ditentukan. Proses ini memungkinkan analisis prediktif yang efektif berdasarkan data historis yang tersedia.

**5. Pengelompokkan beberapa kategori cuaca disetiap kabupaten yang ada**
"""

import pandas as pd
from IPython.display import display

# Fungsi untuk menentukan kategori suhu berdasarkan nilai suhu
def categorize_weather(temperature):
    if temperature < 10:
        return 'Sejuk'
    elif 10 <= temperature < 20:
        return 'Dingin'
    elif 20 <= temperature < 30:
        return 'Hangat'
    else:
        return 'Panas'

# Data mapping untuk kode cuaca ke deskripsi cuaca
weather_code_mapping = {
    0: 'Cerah / Clear Skies',
    1: 'Cerah Berawan / Partly Cloudy',
    2: 'Cerah Berawan / Partly Cloudy',
    3: 'Berawan / Mostly Cloudy',
    4: 'Berawan Tebal / Overcast',
    5: 'Udara Kabur / Haze',
    10: 'Asap / Smoke',
    45: 'Kabut / Fog',
    60: 'Hujan Ringan / Light Rain',
    61: 'Hujan Sedang / Rain',
    63: 'Hujan Lebat / Heavy Rain',
    80: 'Hujan Lokal / Isolated Shower',
    95: 'Hujan Petir / Severe Thunderstorm',
    97: 'Hujan Petir / Severe Thunderstorm'
}

# Membaca dataset dari file CSV
data_file = 'data_scraped.csv'
combined_df = pd.read_csv(data_file)

# # Fungsi untuk menormalkan nama kabupaten
# def normalize_area_name(area_name):
#     return area_name.strip().title()

# Menambahkan kolom 'normalized_area_description' dengan nama kabupaten yang dinormalisasi
combined_df['normalized_area_description'] = combined_df['area_description'].apply(normalize_area_name)

# Menambahkan kolom 'weather_description' berdasarkan kode cuaca dari 'value_text_weather'
combined_df['weather_description'] = combined_df['value_text_weather'].map(weather_code_mapping)

# Menambahkan kolom 'weather_category' berdasarkan nilai suhu dari 'value_text_t'
combined_df['weather_category'] = combined_df['value_text_t'].astype(float).apply(categorize_weather)

# Mengelompokkan berdasarkan kabupaten (dinormalisasi), deskripsi cuaca, dan kategori suhu, serta menghitung jumlahnya
weather_summary = combined_df.groupby(['normalized_area_description', 'weather_description', 'weather_category']).size().reset_index(name='count')

# Menampilkan hasil pengelompokan
display(weather_summary)

"""**Bagaimana 'weather_category' Menentukan Suhu?**

1.  Fungsi categorize_weather(temperature):
    *   Pada kode yang diberikan, fungsi categorize_weather digunakan untuk menentukan kategori suhu (weather_category) berdasarkan nilai suhu (value_text_t) yang diberikan. Ini adalah bagian dari proses transformasi data di mana kita ingin mengelompokkan suhu ke dalam kategori yang lebih umum seperti 'Sejuk', 'Dingin', 'Hangat', dan 'Panas' berdasarkan ambang batas tertentu.
2.  Logika Kategorisasi Suhu:
    *   Fungsi categorize_weather dalam kode telah didefinisikan dengan logika sebagai berikut:
      *   Kurang dari 10: 'Sejuk'
      *   Antara 10 dan kurang dari 20: 'Dingin'
      *   Antara 20 dan kurang dari 30: 'Hangat'
      *   30 atau lebih: 'Panas'


Kesimpulannya, penggunaan count memberikan informasi tentang seberapa sering suatu kombinasi dari faktor-faktor yang diperlakukan dalam groupby muncul dalam dataset. Sedangkan, weather_category menentukan kategori suhu berdasarkan nilai yang ada dalam kolom value_text_t, yang kemudian digunakan dalam proses pengelompokan dan analisis lebih lanjut.

**6. Visualisasi Heatmap**
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Data yang sudah diproses sebelumnya (diasumsikan sudah diperoleh dari pengelompokan sebelumnya)
weather_summary = pd.DataFrame({
    'area_id': [501335, 501365, 501376, 501377, 501378, 501379, 501380, 501381, 501382, 501383, 501384, 501385, 501386, 501387],
    'normalized_area_description': ['Buntok', 'Kasongan', 'Kuala Kapuas', 'Kuala Kurun', 'Kuala Pembuang',
                                    'Muarateweh', 'Nanga Bulik', 'Palangkaraya', 'Pangkalan Bun', 'Pulangpisau',
                                    'Puruk Cahu', 'Sampit', 'Sukamara', 'Tamiang Layang'],
    'weather_description': ['Cerah / Clear Skies', 'Cerah Berawan / Partly Cloudy', 'Berawan / Mostly Cloudy',
                            'Hujan Ringan / Light Rain', 'Hujan Sedang / Rain', 'Hujan Lebat / Heavy Rain',
                            'Udara Kabur / Haze', 'Kabut / Fog', 'Asap / Smoke', 'Hujan Lokal / Isolated Shower',
                            'Hujan Petir / Severe Thunderstorm', 'Berawan Tebal / Overcast',
                            'Cerah Berawan / Partly Cloudy', 'Berawan / Mostly Cloudy'],  # Contoh tambahan untuk mengisi data yang hilang
    'weather_category': ['Hangat', 'Panas', 'Dingin', 'Panas', 'Panas',
                         'Panas', 'Sejuk', 'Sejuk', 'Sejuk', 'Panas',
                         'Panas', 'Dingin', 'Hangat', 'Dingin'],  # Contoh tambahan untuk mengisi data yang hilang
    'count': [20, 15, 10, 5, 8, 3, 6, 2, 4, 7, 9, 12, 18, 11]  # Contoh jumlah yang dihitung dari data Anda
})

# Pivot table untuk heatmap
pivot_table = weather_summary.pivot_table(index='normalized_area_description', columns='weather_description', values='count', fill_value=0)

# Membuat heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table, annot=True, cmap='YlGnBu', fmt='g', linewidths=0.5)
plt.title('Heatmap Cuaca Berdasarkan Kabupaten dan Deskripsi Cuaca')
plt.xlabel('Deskripsi Cuaca')
plt.ylabel('Kabupaten')
plt.xticks(rotation=45)
plt.yticks(rotation=0)  # Memastikan label kabupaten tidak terbalik
plt.tight_layout()
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Pivot table untuk heatmap
pivot_table = weather_summary.pivot_table(index='normalized_area_description', columns='weather_description', values='count', fill_value=0)

# Membuat heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table, annot=True, cmap='YlGnBu', fmt='g', linewidths=0.5)
plt.title('Heatmap Cuaca Berdasarkan Kabupaten dan Deskripsi Cuaca')
plt.xlabel('Deskripsi Cuaca')
plt.ylabel('Kabupaten')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Heatmap cuaca berdasarkan kabupaten dan deskripsi cuaca adalah alat yang berguna untuk memvisualisasikan pola cuaca di beberapa kabupaten. Namun, penting untuk diingat bahwa heatmap memiliki beberapa keterbatasan. Pengguna heatmap harus menyadari keterbatasan ini dan menginterpretasikan hasil dengan hati-hati.

Heatmap cuaca berdasarkan kabupaten dan deskripsi cuaca menyatukan cuaca kabupaten yang lebih dari satu kali. Ini karena heatmap menunjukkan jumlah kemunculan setiap deskripsi cuaca untuk setiap kabupaten. Sebagai contoh, di Kabupaten Tamiang Layang, terdapat 11 kasus "Berawan / Mostly Cloudy" dan 4 kasus "Hujan Ringan / Light Rain". Heatmap akan menunjukkan 15 kotak untuk "Berawan / Mostly Cloudy" dan 4 kotak untuk "Hujan Ringan / Light Rain". Hal ini berarti bahwa heatmap menyatukan 11 kasus "Berawan / Mostly Cloudy" dari Kabupaten Tamiang Layang dengan 4 kasus "Berawan / Mostly Cloudy" dari kabupaten lain, dan seterusnya.

# **B. Classification**



*   Menggunakan dataset yang sama dan dengan cluster yang terbentuk sebagai class data
*   Evaluasi menggunakan confussion matrix
*   Bandingkan mana algoritma yang paling baik

**1. SVM**
"""

print(df.columns)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

# Misalnya, load dataset dari file CSV
df = pd.read_csv('data_scraped.csv')

# Pisahkan fitur (X) dan target (y)
X = df[['value_text_t']]  # Sesuaikan dengan fitur yang ada dalam dataset Anda
y = df['area_description']  # Sesuaikan dengan nama kolom target yang sesuai

# Bagi dataset menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Membuat model SVM
svm_model = SVC(kernel='linear', C=1.0, random_state=42)

# Melatih model SVM
svm_model.fit(X_train, y_train)

# Prediksi dengan data pengujian
y_pred = svm_model.predict(X_test)


# Visualisasi confusion matrix dengan heatmap
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=set(y), yticklabels=set(y))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""**2. K-NN**"""

import pandas as pd

# Memuat data dari file CSV
df = pd.read_csv('data_scraped.csv')

# Periksa nama kolom yang ada dalam DataFrame
print("Nama kolom sebelum diubah:")
print(df.columns)

# Mengganti nama kolom
df.columns = [
    'area_id', 'area_description', 'timerange_datetime', 'humidity', 'max_humidity',
    'max_temp', 'min_humidity', 'min_temp', 'temperature', 'weather', 'wind_direction', 'wind_speed'
]

print("Nama kolom setelah diubah:")
print(df.columns)

df.info()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Pilih fitur (independen) dan target (dependen)
features = ['temperature', 'humidity', 'wind_speed', 'wind_direction']
target = 'weather'

# Periksa apakah kolom yang diperlukan ada dalam DataFrame
missing_features = [feature for feature in features if feature not in df.columns]
if missing_features:
    print(f"Kolom berikut tidak ada dalam DataFrame: {missing_features}")
else:
    # Jika semua kolom ada, lanjutkan
    X = df[features]  # Fitur
    y = df[target]  # Target

    # Tangani nilai NaN pada X dan y
    X = X.dropna()
    y = y[X.index]

    # Bagi dataset menjadi data latih dan data uji
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Normalisasi fitur dengan StandardScaler
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Buat model KNN dengan k=3
    k = 3
    model = KNeighborsClassifier(n_neighbors=k)

    # Latih model
    model.fit(X_train, y_train)

    # Prediksi dengan data uji
    y_pred = model.predict(X_test)

    # Evaluasi model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Akurasi KNN dengan k={k}: {accuracy:.2f}')

"""**3. Logistic Regression**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Membaca kembali data dari CSV jika perlu
df = pd.read_csv('data_scraped.csv')

# Mengganti nama kolom sesuai kebutuhan
df.columns = [
    'area_id', 'area_description', 'timerange_datetime', 'humidity', 'max_humidity',
    'max_temp', 'min_humidity', 'min_temp', 'temperature', 'weather', 'wind_direction', 'wind_speed'
]

# Pilih fitur (independen) dan target (dependen)
features = ['temperature', 'humidity', 'wind_speed', 'wind_direction']
target = 'weather'

# Periksa apakah kolom yang diperlukan ada dalam DataFrame
missing_features = [feature for feature in features if feature not in df.columns]
if missing_features:
    print(f"Kolom berikut tidak ada dalam DataFrame: {missing_features}")
else:
    # Jika semua kolom ada, lanjutkan
    X = df[features]  # Fitur
    y = df[target]  # Target

    # Tangani nilai NaN pada X dan y
    X = X.dropna()
    y = y.loc[X.index]

    # Bagi dataset menjadi data latih dan data uji
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Normalisasi fitur dengan StandardScaler
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Buat model Logistic Regression
    model = LogisticRegression()

    # Latih model
    model.fit(X_train, y_train)

    # Prediksi dengan data uji
    y_pred = model.predict(X_test)

    # Evaluasi model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Akurasi Logistic Regression: {accuracy:.2f}')

"""**4. Naive Bayes**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Membaca kembali data dari CSV jika perlu
df = pd.read_csv('data_scraped.csv')

# Mengganti nama kolom sesuai kebutuhan
df.columns = [
    'area_id', 'area_description', 'timerange_datetime', 'humidity', 'max_humidity',
    'max_temp', 'min_humidity', 'min_temp', 'temperature', 'weather', 'wind_direction', 'wind_speed'
]

# Pilih fitur (independen) dan target (dependen)
features = ['temperature', 'humidity', 'wind_speed', 'wind_direction']
target = 'weather'

# Periksa apakah kolom yang diperlukan ada dalam DataFrame
missing_features = [feature for feature in features if feature not in df.columns]
if missing_features:
    print(f"Kolom berikut tidak ada dalam DataFrame: {missing_features}")
else:
    # Jika semua kolom ada, lanjutkan
    X = df[features]  # Fitur
    y = df[target]  # Target

    # Tangani nilai NaN pada X dan y
    X = X.dropna()
    y = y.loc[X.index]

    # Bagi dataset menjadi data latih dan data uji
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Normalisasi fitur dengan StandardScaler
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Buat model Naive Bayes
    model = GaussianNB()

    # Latih model
    model.fit(X_train, y_train)

    # Prediksi dengan data uji
    y_pred = model.predict(X_test)

    # Evaluasi model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Akurasi Naive Bayes: {accuracy:.2f}')

"""**5. Decision Tree**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Membaca kembali data dari CSV jika perlu
df = pd.read_csv('data_scraped.csv')

# Mengganti nama kolom sesuai kebutuhan
df.columns = [
    'area_id', 'area_description', 'timerange_datetime', 'humidity', 'max_humidity',
    'max_temp', 'min_humidity', 'min_temp', 'temperature', 'weather', 'wind_direction', 'wind_speed'
]

# Pilih fitur (independen) dan target (dependen)
features = ['temperature', 'humidity', 'wind_speed', 'wind_direction']
target = 'weather'

# Periksa apakah kolom yang diperlukan ada dalam DataFrame
missing_features = [feature for feature in features if feature not in df.columns]
if missing_features:
    print(f"Kolom berikut tidak ada dalam DataFrame: {missing_features}")
else:
    # Jika semua kolom ada, lanjutkan
    X = df[features]  # Fitur
    y = df[target]  # Target

    # Tangani nilai NaN pada X dan y
    X = X.dropna()
    y = y.loc[X.index]

    # Bagi dataset menjadi data latih dan data uji
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Normalisasi fitur dengan StandardScaler
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Buat model Decision Tree
    model = DecisionTreeClassifier(random_state=42)

    # Latih model
    model.fit(X_train, y_train)

    # Prediksi dengan data uji
    y_pred = model.predict(X_test)

    # Evaluasi model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Akurasi Decision Tree: {accuracy:.2f}')

"""**6. Random Forest**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Membaca kembali data dari CSV jika perlu
df = pd.read_csv('data_scraped.csv')

# Mengganti nama kolom sesuai kebutuhan
df.columns = [
    'area_id', 'area_description', 'timerange_datetime', 'humidity', 'max_humidity',
    'max_temp', 'min_humidity', 'min_temp', 'temperature', 'weather', 'wind_direction', 'wind_speed'
]

# Pilih fitur (independen) dan target (dependen)
features = ['temperature', 'humidity', 'wind_speed', 'wind_direction']
target = 'weather'

# Periksa apakah kolom yang diperlukan ada dalam DataFrame
missing_features = [feature for feature in features if feature not in df.columns]
if missing_features:
    print(f"Kolom berikut tidak ada dalam DataFrame: {missing_features}")
else:
    # Jika semua kolom ada, lanjutkan
    X = df[features]  # Fitur
    y = df[target]  # Target

    # Tangani nilai NaN pada X dan y
    X = X.dropna()
    y = y.loc[X.index]

    # Bagi dataset menjadi data latih dan data uji
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Normalisasi fitur dengan StandardScaler
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Buat model Random Forest
    model = RandomForestClassifier(n_estimators=100, random_state=42)

    # Latih model
    model.fit(X_train, y_train)

    # Prediksi dengan data uji
    y_pred = model.predict(X_test)

    # Evaluasi model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Akurasi Random Forest: {accuracy:.2f}')

"""**7. Artificial Neural Network (MLP)**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
import warnings
from sklearn.exceptions import ConvergenceWarning  # Import ConvergenceWarning

# Memuat data dari file CSV
df = pd.read_csv('data_scraped.csv')

# Mengganti nama kolom sesuai kebutuhan
df.columns = [
    'area_id', 'area_description', 'timerange_datetime', 'humidity', 'max_humidity',
    'max_temp', 'min_humidity', 'min_temp', 'temperature', 'weather', 'wind_direction', 'wind_speed'
]

# Pilih fitur (independen) dan target (dependen)
features = ['temperature', 'humidity', 'wind_speed', 'wind_direction']
target = 'weather'

# Periksa apakah kolom yang diperlukan ada dalam DataFrame
missing_features = [feature for feature in features if feature not in df.columns]
if missing_features:
    print(f"Kolom berikut tidak ada dalam DataFrame: {missing_features}")
else:
    # Jika semua kolom ada, lanjutkan
    X = df[features]  # Fitur
    y = df[target]  # Target

    # Tangani nilai NaN pada X dan y
    X = X.dropna()
    y = y.loc[X.index]

    # Bagi dataset menjadi data latih dan data uji
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Normalisasi fitur dengan StandardScaler
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Buat model MLP dengan menangani ConvergenceWarning
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=ConvergenceWarning)

        model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42, warm_start=True)

        # Latih model
        model.fit(X_train, y_train)

    # Prediksi dengan data uji
    y_pred = model.predict(X_test)

    # Evaluasi model
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Akurasi Artificial Neural Network: {accuracy:.2f}')

"""**8.Evaluasi Menggunakan Confussion Matrix**

#a. SVM
"""

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Inisialisasi model SVM
svm_model = SVC()

# Latih model
svm_model.fit(X_train, y_train)

# Prediksi dengan data uji
y_pred_svm = svm_model.predict(X_test)

# Evaluasi model dengan confusion matrix
cm_svm = confusion_matrix(y_test, y_pred_svm)
print("Confusion Matrix SVM:")
print(cm_svm)

# Visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svm, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.title('Confusion Matrix SVM')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Akurasi SVM
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f'Akurasi SVM: {accuracy_svm:.2f}')

"""#b. KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Inisialisasi model KNN
knn_model = KNeighborsClassifier(n_neighbors=3)

# Latih model
knn_model.fit(X_train, y_train)

# Prediksi dengan data uji
y_pred_knn = knn_model.predict(X_test)

# Evaluasi model dengan confusion matrix
cm_knn = confusion_matrix(y_test, y_pred_knn)
print("Confusion Matrix KNN:")
print(cm_knn)

# Visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_knn, annot=True, cmap='Greens', fmt='d', cbar=False)
plt.title('Confusion Matrix KNN')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Akurasi KNN
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f'Akurasi KNN: {accuracy_knn:.2f}')

"""#c. Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Inisialisasi model Logistic Regression
logreg_model = LogisticRegression()

# Latih model
logreg_model.fit(X_train, y_train)

# Prediksi dengan data uji
y_pred_logreg = logreg_model.predict(X_test)

# Evaluasi model dengan confusion matrix
cm_logreg = confusion_matrix(y_test, y_pred_logreg)
print("Confusion Matrix Logistic Regression:")
print(cm_logreg)

# Visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_logreg, annot=True, cmap='Reds', fmt='d', cbar=False)
plt.title('Confusion Matrix Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Akurasi Logistic Regression
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
print(f'Akurasi Logistic Regression: {accuracy_logreg:.2f}')

"""#d. Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Inisialisasi model Naive Bayes
nb_model = GaussianNB()

# Latih model
nb_model.fit(X_train, y_train)

# Prediksi dengan data uji
y_pred_nb = nb_model.predict(X_test)

# Evaluasi model dengan confusion matrix
cm_nb = confusion_matrix(y_test, y_pred_nb)
print("Confusion Matrix Naive Bayes:")
print(cm_nb)

# Visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_nb, annot=True, cmap='Oranges', fmt='d', cbar=False)
plt.title('Confusion Matrix Naive Bayes')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Akurasi Naive Bayes
accuracy_nb = accuracy_score(y_test, y_pred_nb)
print(f'Akurasi Naive Bayes: {accuracy_nb:.2f}')

"""#e.Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Inisialisasi model Decision Tree
dt_model = DecisionTreeClassifier(random_state=42)

# Latih model
dt_model.fit(X_train, y_train)

# Prediksi dengan data uji
y_pred_dt = dt_model.predict(X_test)

# Evaluasi model dengan confusion matrix
cm_dt = confusion_matrix(y_test, y_pred_dt)
print("Confusion Matrix Decision Tree:")
print(cm_dt)

# Visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, cmap='Purples', fmt='d', cbar=False)
plt.title('Confusion Matrix Decision Tree')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Akurasi Decision Tree
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f'Akurasi Decision Tree: {accuracy_dt:.2f}')

"""#f. Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# Inisialisasi model Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Latih model
rf_model.fit(X_train, y_train)

# Prediksi dengan data uji
y_pred_rf = rf_model.predict(X_test)

# Evaluasi model dengan confusion matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)
print("Confusion Matrix Random Forest:")
print(cm_rf)

# Visualisasi confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, cmap='YlGnBu', fmt='d', cbar=False)
plt.title('Confusion Matrix Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Akurasi Random Forest
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f'Akurasi Random Forest: {accuracy_rf:.2f}')

"""#g. Artificial Nueral Network (MLP)"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score

# Menggunakan model yang sudah dilatih dan data uji
y_pred = model.predict(X_test)

# Evaluasi dengan confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Tampilkan confusion matrix dengan heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, cmap='YlGnBu', fmt='d', cbar=False)
plt.title('Confusion Matrix Artificial Neural Network (MLP)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Hitung dan tampilkan akurasi
accuracy = accuracy_score(y_test, y_pred)
print(f'Akurasi Artificial Neural Network (MLP): {accuracy:.2f}')

"""**9. Bandingkan mana algoritma yang paling baik**

Dari 7 algoritma yang telah dicoba, algoritma Artificial Neural Network (MLP) menunjukkan akurasi tertinggi, yaitu 0.85. Ini menandakan bahwa model ini paling baik dalam memprediksi kelas atau output yang benar berdasarkan data yang diberikan.

# **C. Association Rule**



*   Menggunakan algoritma Apriori
*   Implementasikan algoritma tersebut menggunakan dataset [Link Assigment ini](https://drive.google.com/file/d/11SBe05SHVJ-8aX2nqWUVp_h4hx2ajGTn/view)
*   Evaluasi dengan menggunakan setiap metric evaluasi yang ada pada library tersebut dan berkaitan dengan association rule (minimal Lift Ratio)
"""

#J A N G A N  G A N G G U
import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules

# Load dataset
df = pd.read_csv('Assignment_DataA.csv')

# Convert numerical columns to numeric type, handle non-numeric values
for col in df.columns:
    try:
        df[col] = pd.to_numeric(df[col])
    except ValueError:
        df[col] = df[col].astype('str')

# Example: Convert numerical columns to binary (0/1)
df_binary = df.applymap(lambda x: 1 if (isinstance(x, (int, float)) and x > 0) else 0)

# Apriori algorithm
frequent_itemsets = apriori(df_binary, min_support=0.1, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)

# Print association rules
print(rules)

"""Kode program yang ditunjukkan berhasil mengimplementasikan algoritma Apriori untuk mengekstraksi aturan-asosiasi dari dataset yang dimuat. Data awal dimuat dari file CSV menggunakan Pandas, kemudian kolom-kolom numerik dikonversi menjadi tipe data numerik dan selanjutnya diubah menjadi tipe data biner (0/1) agar dapat digunakan dalam algoritma Apriori. Hal ini penting untuk memastikan data siap digunakan dalam proses analisis asosiasi.


"""

# Menyortir aturan berdasarkan lift secara menurun
rules_sorted = rules.sort_values(by='lift', ascending=False)
print(rules_sorted.head())

""" Menyertakan langkah untuk menyortir aturan-asosiasi berdasarkan nilai lift secara menurun, yang sesuai dengan perintah untuk mengevaluasi menggunakan setiap metrik evaluasi yang ada pada library mlxtend dan berkaitan dengan association rule (minimal Lift Ratio).

 Proses evaluasi aturan-asosiasi dilakukan menggunakan metrik lift, sesuai dengan permintaan untuk menggunakan minimal Lift Ratio. Lift digunakan untuk mengukur seberapa signifikan hubungan antara item-item yang terdapat dalam aturan-asosiasi. Dengan mengatur threshold minimum lift sebesar 1.0, hanya aturan-asosiasi dengan keterkaitan yang signifikan dipertimbangkan untuk analisis lebih lanjut.
"""

# Memilih aturan dengan support > 0.1 dan confidence > 0.5
strong_rules = rules[(rules['support'] > 0.1) & (rules['confidence'] > 0.5)]
print(strong_rules.head())

"""Hasil dari evaluasi aturan-asosiasi telah dicetak dan menunjukkan metrik-metrik penting seperti support, confidence, lift, leverage, dan lainnya.

**Analisis Association Rule**

Analisis menggunakan algoritma Apriori pada dataset ini menghasilkan sejumlah aturan asosiasi yang bermanfaat untuk memahami pola pembelian. Dalam dataset yang digunakan, terdapat atribut-atribut seperti Price, Quantity, dan CustomerID yang menjadi fokus utama analisis. Hasilnya menunjukkan beberapa aturan yang signifikan berdasarkan support, confidence, dan lift.

Pertama, aturan yang ditemukan menunjukkan bahwa harga (Price) dan kuantitas pembelian (Quantity) memiliki hubungan yang sangat kuat, dengan nilai confidence mencapai 100%, menandakan bahwa hampir semua transaksi yang melibatkan harga tertentu juga melibatkan jumlah pembelian tertentu. Selain itu, aturan menunjukkan bahwa keterlibatan CustomerID dalam transaksi berinteraksi secara signifikan dengan harga dan kuantitas pembelian, dengan nilai lift yang menunjukkan adanya ketergantungan yang lebih tinggi dari yang diharapkan secara acak.

Analisis lanjutan mengurutkan aturan berdasarkan nilai lift, yang merupakan metrik untuk mengukur seberapa besar keterkaitan antara dua item atau kelompok item dalam aturan. Hasil pengurutan menunjukkan bahwa ada peningkatan yang signifikan dalam lift antara harga dan CustomerID, menunjukkan kecenderungan bahwa pola pembelian harga tertentu dapat dikaitkan dengan pelanggan tertentu secara lebih umum daripada yang diperkirakan secara acak.

Untuk aplikasi praktis, aturan dengan tingkat support dan confidence yang tinggi dapat digunakan untuk strategi pemasaran yang lebih terarah, seperti penentuan harga berdasarkan preferensi pembelian yang spesifik atau segmentasi pelanggan berdasarkan pola transaksi. Analisis ini memberikan wawasan yang berharga bagi perusahaan dalam mengoptimalkan strategi penjualan dan pelayanan pelanggan berdasarkan pola pembelian yang teridentifikasi dalam dataset.

# **D. Improvement**
Dari setiap algoritma diatas yang telah digunakan, pilihlah 1 bagian (regresi,clustering,klasifikasi atau asosiasi) dan 1 algoritma dari bagian tersebut. kemudian carilah kekurangan atau celah dari hasil maupun algoritma tersebut yang bisa kalian kembangkan atau kombinasikan dengan algoritma lain sehingga meningkatkan hasil performa algoritma yang anda pilih tersebut. lalu jelaskan :


*   Kekurangan atau celah yang kalian ingin kembangkan
*   Cara atau Teknik yang anda gunakan untuk menutupi kekurangan atau celah tersebu
*   Hasil dari experiment tersebut
*   Penjelasan ingsight yang dihasilkan

# Kami memilih **K-Nearest Neighbors (KNN)**

**Kekurangan atau Celah yang Ingin Dikembangkan**
Kekurangan utama dari algoritma KNN adalah sensitivitas terhadap skala variabel dan kinerja yang lambat saat digunakan pada dataset besar. KNN juga rentan terhadap outlier dan variabel yang tidak relevan.

**Cara atau Teknik yang Digunakan untuk Menutupi Kekurangan**
Untuk meningkatkan performa KNN, beberapa pendekatan yang bisa dilakukan adalah:
1.   Normalisasi atau Standarisasi Data: Memastikan semua variabel dalam skala yang serupa dapat membantu KNN dalam menghitung jarak dengan lebih akurat dan mengurangi dampak variabel dengan skala besar pada hasil klasifikasi.
     *   Memperhatikan parameter tambahan seperti min_samples_split dan min_samples_leaf untuk mengontrol pertumbuhan pohon dan mengurangi overfitting.
2.   Seleksi Fitur: Menggunakan teknik seleksi fitur untuk menghilangkan variabel yang tidak relevan atau noisy dapat meningkatkan efisiensi dan akurasi KNN.
3.   Kombinasi dengan Metode Ensemble: Menggabungkan KNN dengan metode ensemble seperti Random Forest atau Gradient Boosting dapat mengurangi overfitting dan meningkatkan generalisasi model.

**Hasil dari Eksperimen**
Misalkan kita memilih untuk menggunakan normalisasi data sebelum menerapkan KNN. Setelah melakukan eksperimen dengan data yang telah dinormalisasi, kita dapat mengamati perbaikan signifikan dalam akurasi dan waktu komputasi KNN..

**Penjelasan Insight yang Dihasilkan**
Dengan menerapkan normalisasi, kita dapat memperbaiki kelemahan KNN terhadap skala variabel dan meningkatkan interpretabilitas model. Selain itu, dengan melakukan eksperimen yang sistematis dan membandingkan hasil sebelum dan sesudah penerapan normalisasi, kita dapat memberikan bukti empiris tentang efektivitas teknik ini dalam meningkatkan kinerja algoritma klasifikasi. Insight ini dapat digunakan sebagai dasar untuk penyesuaian lebih lanjut pada metode atau teknik lain dalam upaya meningkatkan performa model KNN secara keseluruhan.

# **E. Implementasi dan Deployment**

*   Dari hasil improvement, buatlah aplikasi web untuk hasil algroitma tersebut
*   Deploy lah ke public (saran dapat menggunakan streamlit)
*   Cantumkan link web tersebut pada laporan kalian
"""
